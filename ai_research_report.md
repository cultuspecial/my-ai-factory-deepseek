# AI 简报 (2026-01-29)

### Evolutionary Strategies lead to Catastrophic Forgetting in LLMs
📄 论文核心贡献：揭示了进化策略（ES）作为大语言模型（LLMs）的无梯度训练方法，虽能在数学推理任务上接近GRPO的性能，但会随着更新步数增加导致严重的灾难性遗忘问题。
🔗 http://arxiv.org/abs/2601.20861v1

### SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models
📄 该论文提出SokoBench基准，通过简化版推箱子游戏评估大语言模型的长程规划能力，发现模型在超过25步的规划任务中性能显著下降，并验证了结合PDDL工具能提升其规划效果。
🔗 http://arxiv.org/abs/2601.20856v1

### Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation
📄 本文通过实验探索了在变分自编码器（VAE）中不同位置引入Transformer对表格数据生成效果的影响，发现将Transformer置于潜在空间和解码器部分能在生成保真度与多样性之间取得平衡，且不同架构在潜在空间表示上具有高度相似性。
🔗 http://arxiv.org/abs/2601.20854v1

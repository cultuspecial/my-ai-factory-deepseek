# AI 简报 (2026-02-12)

### Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling
📄 本文提出了一种基于扩散模型原生潜空间的奖励建模方法（DiNa-LRM），通过直接在噪声扩散状态上进行偏好学习，避免了传统视觉语言模型（VLM）奖励在计算成本高和像素域不匹配的问题，并引入噪声校准的Thurstone似然函数来提升对齐效果。
🔗 http://arxiv.org/abs/2602.11146v1

### GENIUS: Generative Fluid Intelligence Evaluation Suite
📄 GENIUS评估套件首次系统评估了统一多模态模型的生成性流体智能，即模型在未见过的场景中归纳隐含模式、执行即时约束并动态适应新任务的能力，弥补了现有基准仅关注依赖已知知识的晶体智能的不足。
🔗 http://arxiv.org/abs/2602.11144v1

### Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows
📄 本文提出了一种基于标准化流的层次化隐式Q学习框架（NF-HIQL），通过使用表达能力更强的标准化流策略替代传统的高斯策略，提升了策略的多模态建模能力与数据效率，并给出了显式的KL散度理论保证。
🔗 http://arxiv.org/abs/2602.11142v1

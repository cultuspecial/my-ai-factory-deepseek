# AI 简报 (2026-01-29)

### Evolutionary Strategies lead to Catastrophic Forgetting in LLMs
📄 论文核心贡献：揭示了进化策略（ES）作为大语言模型（LLMs）的无梯度训练方法，虽能在数学推理任务上达到接近GRPO的性能，但会随着更新步数增加而出现严重的灾难性遗忘问题。
🔗 http://arxiv.org/abs/2601.20861v1

### SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models
📄 SokoBench 评估发现大型语言模型在需要超过 25 步的长程规划任务中性能显著下降，并证明结合 PDDL 等规划工具可有效提升其规划能力。
🔗 http://arxiv.org/abs/2601.20856v1

### Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation
📄 本文通过实验探究了在变分自编码器（VAE）中引入Transformer模块对表格数据生成效果的影响，发现将Transformer置于VAE的潜在空间和解码器部分能够更好地建模特征间复杂关系，但会在生成结果的保真度与多样性之间产生权衡，同时不同数据集的生成质量差异显著。
🔗 http://arxiv.org/abs/2601.20854v1

# AI 简报 (2026-01-29)

### Evolutionary Strategies lead to Catastrophic Forgetting in LLMs
📄 论文贡献：本文首次全面分析了进化策略（ES）在大型语言模型（LLMs）持续学习中的表现，发现尽管ES在数学和推理任务上能达到接近GRPO的性能，但随着更新步数增加，ES会导致严重的灾难性遗忘问题。
🔗 http://arxiv.org/abs/2601.20861v1

### SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models
📄 SokoBench 通过简化推箱子游戏评估大语言模型的长程规划能力，发现其规划能力在超过25步时显著下降，而结合PDDL工具可提升性能。
🔗 http://arxiv.org/abs/2601.20856v1

### Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation
📄 本文通过实验探索在变分自编码器（VAE）中不同位置引入Transformer对表格数据生成的影响，发现将Transformer置于隐空间和解码器部分能在生成保真度与多样性之间取得平衡，且不同配置下模型性能相似。
🔗 http://arxiv.org/abs/2601.20854v1

# AI 简报 (2026-01-29)

### Evolutionary Strategies lead to Catastrophic Forgetting in LLMs
📄 论文贡献：本文通过全面分析进化策略（ES）在大语言模型（LLMs）中的持续学习表现，首次揭示ES在数学与推理任务上虽能达到接近GRPO的性能，但随着更新步数增加会引发灾难性遗忘，突显了ES作为无梯度学习方法在持续学习场景中的关键局限性。
🔗 http://arxiv.org/abs/2601.20861v1

### SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models
📄 SokoBench 通过简化版推箱子游戏评估大语言模型的长期规划能力，发现模型在需超过25步的规划任务中性能显著下降，而引入PDDL工具链可有效提升规划效果。
🔗 http://arxiv.org/abs/2601.20856v1

### Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation
📄 本文通过实验探究了在变分自编码器（VAE）中不同位置引入Transformer对表格数据生成效果的影响，发现将Transformer置于潜空间和解码器部分能在生成保真度与多样性之间取得平衡，且不同数据集的性能表现呈现高度相似性。
🔗 http://arxiv.org/abs/2601.20854v1

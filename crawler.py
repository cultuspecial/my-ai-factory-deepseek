# crawler.py - å¢å¼ºç‰ˆï¼šçœŸå®æŠ“å– arXiv è®ºæ–‡
import json
import requests
import re
from datetime import datetime, timedelta

def crawl_arxiv_ai_papers(max_results=10, lookback_days=7):
    """
    çœŸå®æŠ“å– arXiv ä¸Š AI ç›¸å…³çš„æœ€æ–°è®ºæ–‡
    è¿”å›æ ¼å¼ä¸æ—§ç‰ˆæœ¬å…¼å®¹çš„åˆ—è¡¨
    """
    print(f"ğŸ” å¼€å§‹æŠ“å– arXiv æœ€æ–° AI è®ºæ–‡ (è¿‘{lookback_days}å¤©)...")
    
    # arXiv API æŸ¥è¯¢å‚æ•°
    categories = ['cs.AI', 'cs.CL', 'cs.LG']  # äººå·¥æ™ºèƒ½ã€è®¡ç®—è¯­è¨€å­¦ã€æœºå™¨å­¦ä¹ 
    query = ' OR '.join([f'cat:{cat}' for cat in categories])
    
    params = {
        'search_query': query,
        'start': 0,
        'max_results': max_results,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    try:
        # è°ƒç”¨ arXiv API
        response = requests.get(
            'http://export.arxiv.org/api/query',
            params=params,
            headers={'User-Agent': 'AI-Research-Bot/1.0'},
            timeout=30
        )
        response.raise_for_status()
        
        # è§£æ XML å“åº”ï¼ˆç®€åŒ–ç‰ˆï¼‰
        import xml.etree.ElementTree as ET
        root = ET.fromstring(response.text)
        
        # XML å‘½åç©ºé—´
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        papers = []
        for entry in root.findall('atom:entry', ns):
            title_elem = entry.find('atom:title', ns)
            summary_elem = entry.find('atom:summary', ns)
            
            if title_elem is not None and summary_elem is not None:
                # æ¸…ç†æ ‡é¢˜å’Œæ‘˜è¦
                title = re.sub(r'\s+', ' ', title_elem.text).strip()
                summary = re.sub(r'\s+', ' ', summary_elem.text).strip()
                
                # æå–è®ºæ–‡ ID
                id_elem = entry.find('atom:id', ns)
                paper_id = id_elem.text.split('/')[-1] if id_elem is not None else 'unknown'
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published_elem = entry.find('atom:published', ns)
                published = published_elem.text if published_elem is not None else ''
                
                papers.append({
                    "title": title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                    "link": f"https://arxiv.org/abs/{paper_id}",
                    "summary": summary[:500],  # æ‘˜è¦å–å‰500å­—ç¬¦ï¼Œä¾›LLMæ€»ç»“
                    "paper_id": paper_id,
                    "published": published[:10] if published else "",
                    "source": "arXiv"
                })
        
        print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers[:max_results]  # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§æ•°é‡
        
    except Exception as e:
        print(f"âš ï¸ arXiv æŠ“å–å¤±è´¥ï¼Œå¯ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼: {e}")
        # é™çº§æ–¹æ¡ˆï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼Œä¿è¯æµæ°´çº¿ä¸ä¸­æ–­
        return [
            {
                "title": "Large Language Models: A Survey",
                "link": "https://arxiv.org/abs/2401.00000",
                "summary": "This paper surveys recent advances in large language models.",
                "paper_id": "2401.00000",
                "published": "2024-01-01",
                "source": "simulation"
            },
            {
                "title": "Efficient Fine-tuning Methods for Transformers",
                "link": "https://arxiv.org/abs/2401.00001",
                "summary": "We propose a new parameter-efficient fine-tuning method.",
                "paper_id": "2401.00001",
                "published": "2024-01-02",
                "source": "simulation"
            }
        ]

if __name__ == "__main__":
    # å‚æ•°å¯ä»¥ä¿æŒä¸ä½ åŸå·¥ä½œæµä¸€è‡´
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--topic", default="AI Trends")
    args = parser.parse_args()
    
    # æŠ“å–è®ºæ–‡ï¼ˆé»˜è®¤ä¸º10ç¯‡ï¼Œè¿‘7å¤©ï¼‰
    data = crawl_arxiv_ai_papers(max_results=10, lookback_days=7)
    
    # ä¿æŒä¸åŸæ–‡ä»¶å®Œå…¨ç›¸åŒçš„è¾“å‡ºæ ¼å¼å’Œæ–‡ä»¶å
    with open("raw_data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ æ•°æ®å·²ä¿å­˜è‡³ raw_data.json, å…± {len(data)} æ¡è®°å½•")

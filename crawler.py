import json
import requests
import re

def crawl_arxiv():
    print("ğŸ” [Crawler] æ­£åœ¨è¿æ¥ arXiv API...")
    # æŠ“å– AI é¢†åŸŸæœ€æ–° 5 ç¯‡è®ºæ–‡
    url = "http://export.arxiv.org/api/query?search_query=cat:cs.AI&max_results=5&sortBy=submittedDate&sortOrder=descending"
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # ä½¿ç”¨æ­£åˆ™æå–ï¼Œæ¯” XML åº“æ›´è½»é‡ã€æ›´é€‚åˆ CI ç¯å¢ƒ
        entries = re.findall(r'<entry>(.*?)</entry>', response.text, re.DOTALL)
        
        papers = []
        for entry in entries:
            title = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
            summary = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
            link = re.search(r'<id>(.*?)</id>', entry, re.DOTALL)
            
            if title and summary:
                papers.append({
                    "title": title.group(1).strip().replace('\n', ' '),
                    "summary": summary.group(1).strip().replace('\n', ' ')[:800],
                    "link": link.group(1).strip() if link else "N/A"
                })
        
        print(f"âœ… [Crawler] æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers

    except Exception as e:
        print(f"âš ï¸ [Crawler] æŠ“å–å¤±è´¥: {e}")
        # é™çº§æ•°æ®ï¼Œä¿è¯ pipeline ä¸æ–­
        return [{"title": "API Error Fallback", "summary": "Simulation data.", "link": "https://arxiv.org"}]

if __name__ == "__main__":
    data = crawl_arxiv()
    with open("raw_data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# summarize.py - å¢žå¼ºç‰ˆï¼šçœŸå®žè°ƒç”¨ DeepSeek API æ€»ç»“è®ºæ–‡
import datetime
import json
import os
import sys
import time

def call_deepseek_api(prompt, api_key, base_url="https://api.deepseek.com"):
    """
    è°ƒç”¨ DeepSeek APIï¼ˆå…¼å®¹ Chat Completions æ ¼å¼ï¼‰
    è¿”å›ž (response_text, total_tokens, success)
    """
    import requests
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½AIç ”ç©¶åŠ©ç†ï¼Œè´Ÿè´£ä»Žè®ºæ–‡æ‘˜è¦ä¸­æå–æ ¸å¿ƒä¿¡æ¯ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }
    
    try:
        response = requests.post(
            f"{base_url}/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        
        # æå–å›žå¤å†…å®¹å’Œ token ä½¿ç”¨é‡
        reply = result["choices"][0]["message"]["content"]
        total_tokens = result.get("usage", {}).get("total_tokens", 0)
        
        return reply, total_tokens, True
        
    except Exception as e:
        print(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return f"APIè°ƒç”¨å¤±è´¥: {e}", 0, False

def record_meta(topic, success, fallback, retries, content, tokens=0):
    """
    ä¿æŒä¸Žä½ åŽŸç‰ˆå®Œå…¨ç›¸åŒçš„å…ƒæ•°æ®è®°å½•å‡½æ•°
    """
    meta = {
        "timestamp": datetime.datetime.now().isoformat(),
        "topic": topic,
        "llm_success": success,
        "fallback_used": fallback,
        "retry_count": retries,
        "content_length": len(content),
        "total_tokens": tokens, 
        "status": "HEALTHY" if success and not fallback else "DEGRADED"
    }
    
    with open("run_history.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(meta, ensure_ascii=False) + "\n")
    
    return meta

def generate_report():
    """
    æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼šè¯»å–æŠ“å–çš„è®ºæ–‡ï¼Œè°ƒç”¨ API æ€»ç»“ï¼Œè®°å½•å…ƒæ•°æ®
    """
    # 1. è¯»å– crawler.py ç”Ÿæˆçš„æ•°æ®
    try:
        with open("raw_data.json", "r", encoding="utf-8") as f:
            papers = json.load(f)
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ° raw_data.jsonï¼Œè¯·å…ˆè¿è¡Œ crawler.py")
        record_meta("Error", False, False, 0, "raw_data.json not found", 0)
        return
    
    if not papers:
        print("âš ï¸ raw_data.json ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        papers = [{"title": "AI Trends", "summary": "No data available."}]
    
    # 2. ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å– API é…ç½®ï¼ˆä¸Žä½ çš„ workflow.yml å®Œå…¨ä¸€è‡´ï¼‰
    api_key = os.environ.get("LLM_API_KEY")
    base_url = os.environ.get("LLM_BASE_URL", "https://api.deepseek.com")
    
    if not api_key:
        print("âŒ æœªè®¾ç½® LLM_API_KEY çŽ¯å¢ƒå˜é‡")
        record_meta("Config Error", False, False, 0, "API key not configured", 0)
        return
    
    # 3. å¤„ç†æ¯ä¸€ç¯‡è®ºæ–‡ï¼ˆç¤ºä¾‹ï¼šå¤„ç†å‰2ç¯‡ï¼‰
    all_summaries = []
    total_tokens_used = 0
    success_count = 0
    
    for i, paper in enumerate(papers[:2]):  # å…ˆå¤„ç†2ç¯‡ï¼ŒæŽ§åˆ¶æˆæœ¬
        print(f"ðŸ“ å¤„ç†è®ºæ–‡ {i+1}/{min(2, len(papers))}: {paper['title'][:50]}...")
        
        # æž„å»ºæç¤ºè¯
        prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹å­¦æœ¯è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼š

æ ‡é¢˜ï¼š{paper.get('title', 'N/A')}
æ‘˜è¦ï¼š{paper.get('summary', 'N/A')}

è¯·ç”¨ä¸­æ–‡æä¾›ï¼š
1. ç ”ç©¶ç›®æ ‡ï¼ˆ1å¥è¯ï¼‰
2. æ ¸å¿ƒæ–¹æ³•ï¼ˆ1-2å¥è¯ï¼‰
3. ä¸»è¦å‘çŽ°ï¼ˆ1-2å¥è¯ï¼‰
4. æ½œåœ¨å½±å“ï¼ˆ1å¥è¯ï¼‰"""
        
        # è°ƒç”¨ API
        summary, tokens, success = call_deepseek_api(prompt, api_key, base_url)
        
        if success:
            success_count += 1
            total_tokens_used += tokens
            all_summaries.append({
                "title": paper.get("title", "Untitled"),
                "summary": summary,
                "tokens": tokens
            })
            print(f"   âœ… æ€»ç»“æˆåŠŸï¼Œæ¶ˆè€— {tokens} tokens")
        else:
            print(f"   âŒ æ€»ç»“å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ‘˜è¦")
            # é™çº§ï¼šä½¿ç”¨ç®€å•æ‘˜è¦
            all_summaries.append({
                "title": paper.get("title", "Untitled"),
                "summary": f"è®ºæ–‡æ‘˜è¦ï¼š{paper.get('summary', '')[:100]}...",
                "tokens": 0
            })
    
    # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå†…å®¹
    if all_summaries:
        report_content = "# AI è®ºæ–‡ç ”ç©¶ç®€æŠ¥\n\n"
        report_content += f"*ç”Ÿæˆæ—¶é—´ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}*\n"
        report_content += f"*åˆ†æžè®ºæ–‡æ•°ï¼š{len(all_summaries)}*\n"
        report_content += f"*æˆåŠŸæ€»ç»“ï¼š{success_count}ç¯‡*\n"
        report_content += f"*æ€» Token æ¶ˆè€—ï¼š{total_tokens_used}*\n\n"
        
        for item in all_summaries:
            report_content += f"## {item['title']}\n"
            report_content += f"{item['summary']}\n\n"
            report_content += f"*Tokens: {item['tokens']}*\n\n---\n\n"
        
        # 5. ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        with open("ai_research_report.md", "w", encoding="utf-8") as f:
            f.write(report_content)
        
        print(f"ðŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³ ai_research_report.md")
        
        # 6. è®°å½•å…ƒæ•°æ®ï¼ˆä¸Žä½ çš„åŽ†å²è®°å½•æ ¼å¼å®Œå…¨å…¼å®¹ï¼‰
        meta = record_meta(
            topic="AI Research Papers",
            success=success_count > 0,
            fallback=success_count < len(all_summaries),
            retries=0,
            content=report_content[:100] + "...",  # åªå­˜å¼€å¤´éƒ¨åˆ†
            tokens=total_tokens_used
        )
        
        print(f"âœ… æˆåŠŸè®°å½•å…ƒæ•°æ®ï¼Œæœ¬æ¬¡æ¶ˆè€— {total_tokens_used} tokens")
    else:
        print("âŒ æœªç”Ÿæˆä»»ä½•æ€»ç»“")
        record_meta("AI Research Papers", False, True, 0, "No summaries generated", 0)

if __name__ == "__main__":
    generate_report()
